<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Saeed  Ghorbani | Publications</title>
<meta name="description" content="Saeed Ghorbani, PhD student, Department of Electrical Engineering and Computer Science, York University
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Saeed</span>   <span class="font-weight-bold">Ghorbani</span>
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">APIN</abbr>
         
        <span class="award badge"></span>
    </div>

    <div id="davoodnia2021_b" class="col-sm-8">
        
        <div class="title">Estimating Pose from Pressure Data for Smart Beds withDeep Image-based Pose Estimators</div>
        <div class="author">
             
               Davoodnia, Vandad,      
             
            <em>Ghorbani, Saeed</em>,     
               and
            <a href="http://alietemad.com/" target="_blank">Etemad, Ali</a>
                
        </div>

        <div class="periodical">
            
            <em>Journal of Applied Intelligence</em>
              2021 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                
            <a href="/assets/pdf/Applied_Intelligence_2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
                    
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In-bed pose estimation has shown value in fields  such as  hospital patient  monitoring,  sleep studies,  and  smart  homes.  In  this  this  paper,  we  explore different strategies for detecting body pose from highly ambiguous pressure data, with the aid of pre-existing pose estimators. We examine the performance of pre-trained  pose  estimators  by  using  them  either  directlyor  by  retraining  them  on  two  pressure  datasets.  We also  explore  other  strategies  utilizing  a  learnable  pre-processing  domain  adaptation  step,  which  transforms the vague pressure maps to a representation closer tothe expected input space of common purpose pose estimation modules. Accordingly, we used a fully convolutional network with multiple scales to provide the pose-specific characteristics of the pressure maps to the pre-trained pose estimation module. Our complete analysis of different approaches shows that the combination of learnable pre-processing module along with re-training pre-existing image-based pose estimators on the pressure data is able to overcome issues such as highly vague pressure points to achieve very high pose estimation accuracy.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">ICCASP</abbr>
         
        <span class="award badge"></span>
    </div>

    <div id="davoodnia2021" class="col-sm-8">
        
        <div class="title">In-bed pressure-based pose estimation using image space representation learning</div>
        <div class="author">
             
               Davoodnia, Vandad,      
             
            <em>Ghorbani, Saeed</em>,     
               and
            <a href="http://alietemad.com/" target="_blank">Etemad, Ali</a>
                
        </div>

        <div class="periodical">
            
            <em>In IEEE International Conference on Acoustics, Speech and Signal Processing</em>
              2021 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                
            <a href="https://arxiv.org/pdf/1908.08919.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
                    
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Recent advances in deep pose estimation models have proven to be effective in a wide range of applications such as health monitoring, sports, animations, and robotics. However, pose estimation models fail to generalize when facing images acquired from in-bed pressure sensing systems. In this paper, we address this challenge by presenting a novel end-to-end framework capable of accurately locating body parts from vague pressure data. Our method exploits the idea of equipping an off-the-shelf pose estimator with a deep trainable neural network, which pre-processes and prepares the pressure data for subsequent pose estimation. Our model transforms the ambiguous pressure maps to images containing shapes and structures similar to the common input domain of the pre-existing pose estimation methods. As a result, we show that our model is able to reconstruct unclear body parts, which in turn enables pose estimators to accurately and robustly estimate the pose. We train and test our method on a manually annotated public pressure map dataset using a combination of loss functions. Results confirm the effectiveness of our method by the high visual quality in the generated images and the high pose estimation rates achieved.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">CGF</abbr>
         
        <span class="award badge"></span>
    </div>

    <div id="ghorbani2020b" class="col-sm-8">
        
        <div class="title">Probabilistic Character Motion Synthesis using a Hierarchical Deep Latent Variable Model</div>
        <div class="author">
             
             
            <em>Ghorbani, Saeed</em>,     
              
            <a href="http://www.cse.yorku.ca/~calden/" target="_blank">Wloka, Calden</a>,      
              
            <a href="http://alietemad.com/" target="_blank">Etemad, Ali</a>,      
              
            <a href="https://mbrubake.github.io/" target="_blank">Brubaker, Marcus A.</a>,      
               and
            <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank">Troje, Nikolaus F.</a>
                
        </div>

        <div class="periodical">
            
            <em>Computer Graphics Forum (Symposium on Computer Aanimation)</em>
              2020 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                
            <a href="/assets/pdf/GhorbaniSCA2020.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
                   
            <a href="https://saeed1262.github.io/projects/motionModel/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
             
            <a href="https://www.youtube.com/watch?v=r9F74LcGC0A" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">arXiv</abbr>
         
        <span class="award badge"></span>
    </div>

    <div id="ghorbani2020movi" class="col-sm-8">
        
        <div class="title">MoVi: A Large Multipurpose Motion and Video Dataset</div>
        <div class="author">
             
             
            <em>Ghorbani, Saeed</em>,     
               Mahdaviani, Kimia,      
               Thaler, Anne,      
              
            <a href="http://koerding.com/" target="_blank">Kording, Konrad</a>,      
               Cook, Douglas James,      
              
            <a href="http://www.compneurosci.com/people.html" target="_blank">Blohm, Gunnar</a>,      
               and
            <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank">Troje, Nikolaus F.</a>
                
        </div>

        <div class="periodical">
            
            <em>arXiv:2003.01888</em>
              2020 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                
            <a href="https://arxiv.org/pdf/2003.01888.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
                
            <a href="https://github.com/saeed1262/MoVi-Toolbox" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
               
            <a href="https://www.biomotionlab.ca/movi/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
             
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Human movements are both an area of intense study and the basis of many applications such as character animation. For many applications, it is crucial to identify movements from videos or analyze datasets of movements. Here we introduce a new human Motion and Video dataset MoVi, which we make available publicly. It contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement. In five capture rounds, the same actors and movements were recorded using different hardware systems, including an optical motion capture system, video cameras, and inertial measurement units (IMU). For some of the capture rounds, the actors were recorded when wearing natural clothing, for the other rounds they wore minimal clothing. In total, our dataset contains 9 hours of motion capture data, 17 hours of video data from 4 different points of view (including one hand-held camera), and 6.6 hours of IMU data. In this paper, we describe how the dataset was collected and post-processed; We present state-of-the-art estimates of skeletal motions and full-body shape deformations associated with skeletal motion. We discuss examples for potential studies this dataset could enable.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">ICPR</abbr>
         
        <span class="award badge"></span>
    </div>

    <div id="sepas2020gait" class="col-sm-8">
        
        <div class="title">Gait Recognition using Multi-Scale Partial Representation Transformation with Capsules</div>
        <div class="author">
             
               Sepas-Moghaddam, Alireza,      
             
            <em>Ghorbani, Saeed</em>,     
              
            <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank">Troje, Nikolaus F</a>,      
               and
            <a href="http://alietemad.com/" target="_blank">Etemad, Ali</a>
                
        </div>

        <div class="periodical">
            
            <em>International Conference on Pattern Recognition</em>
              2020 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                
            <a href="https://arxiv.org/pdf/2010.09084.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
                    
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Gait recognition, referring to the identification of individuals based on the manner in which they walk, can be very challenging due to the variations in the viewpoint of the camera and the appearance of individuals. Current methods for gait recognition have been dominated by deep learning models, notably those based on partial feature representations. In this context, we propose a novel deep network, learning to transfer multi-scale partial gait representations using capsules to obtain more discriminative gait features. Our network first obtains multi-scale partial representations using a state-of-the-art deep partial feature extractor. It then recurrently learns the correlations and co-occurrences of the patterns among the partial features in forward and backward directions using Bi-directional Gated Recurrent Units (BGRU). Finally, a capsule network is adopted to learn deeper part-whole relationships and assigns more weights to the more relevant features while ignoring the spurious dimensions. That way, we obtain final features that are more robust to both viewing and appearance changes. The performance of our method has been extensively tested on two gait recognition datasets, CASIA-B and OU-MVLP, using four challenging test protocols. The results of our method have been compared to the state-of-the-art gait recognition solutions, showing the superiority of our model, notably when facing challenging viewing and carrying conditions.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">CGI</abbr>
         
        <span class="award badge">Best Paper Award</span>
    </div>

    <div id="ghorbani2019auto" class="col-sm-8">
        
        <div class="title">Auto-labelling of markers in optical motion capture by permutation learning</div>
        <div class="author">
             
             
            <em>Ghorbani, Saeed</em>,     
              
            <a href="http://alietemad.com/" target="_blank">Etemad, Ali</a>,      
               and
            <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank">Troje, Nikolaus F</a>
                
        </div>

        <div class="periodical">
            
            <em>In Computer Graphics International</em>
              2019 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                
            <a href="https://arxiv.org/pdf/1907.13580.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
                    
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Optical marker-based motion capture is a vital tool in applications such as motion and behavioural analysis, animation, and biomechanics. Labelling, that is, assigning optical markers to the pre-defined positions on the body is a time consuming and labour intensive postprocessing part of current motion capture pipelines. The problem can be considered as a ranking process in which markers shuffled by an unknown permutation matrix are sorted to recover the correct order. In this paper, we present a framework for automatic marker labelling which first estimates a permutation matrix for each individual frame using a differentiable permutation learning model and then utilizes temporal consistency to identify and correct remaining labelling errors. Experiments conducted on the test data show the effectiveness of our framework.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">CVR</abbr>
         
        <span class="award badge">Best Poster Award</span>
    </div>

    <div id="ghorbani2019" class="col-sm-8">
        
        <div class="title">Automatic initialization and tracking of markers in optical motion capture by learning to rank</div>
        <div class="author">
             
             
            <em>Ghorbani, Saeed</em>,     
              
            <a href="http://alietemad.com/" target="_blank">Etemad, Ali</a>,      
               and
            <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank">Troje, Nikolaus F</a>
                
        </div>

        <div class="periodical">
            
            <em>In CVR Vision Conference</em>
              2019 
        </div>
        

        <div class="links">
                    
            <a href="https://www.biomotionlab.ca/Abstracts/Ghorbani_CVR2019.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
                
        </div>

        <!-- Hidden abstract block -->
        
    </div>
</div>
</li></ol>

  <h2 class="year">2010</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
         
        <abbr class="badge">WCSP</abbr>
         
        <span class="award badge"></span>
    </div>

    <div id="ghayoor2010sub" class="col-sm-8">
        
        <div class="title">Sub-pixel image registration based on physical forces</div>
        <div class="author">
             
               Ghayoor, Ali,      
             
            <em>Ghorbani, Saeed</em>,     
               and Shirazi, Ali Asghar Beheshti     
        </div>

        <div class="periodical">
            
            <em>In International Conference on Wireless Communications &amp; Signal Processing (WCSP)</em>
              2010 
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
              
            <a href="https://ieeexplore.ieee.org/document/5633452" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
                    
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>A new method for image registration has been previously proposed by the authors, which the registration is based on physical forces. The registration parameters are translation and rotation. This method assumes images like charged materials that attract each other. In this case, one of the images moves in the same direction as the applied force while the other one is still. The movement of the image continues until the resultant force becomes zero. This approach estimates the registration parameters simultaneously and leading to a better optimized set of registration parameters. The registration error for this method is 1 to 3 pixels. In this paper we aim to develop this method for the applications which need sub-pixel accuracy. First, by applying the Canny edge detector on the input images, the edge information is also used for the registration process to increase the robustness of this method in the presence of noise. After that, sub-pixel accuracy is provided for this method by using interpolation techniques.</p>
        </div>
        
    </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Saeed  Ghorbani.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: April 09, 2021.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-179482637-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-179482637-1');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>



</html>
